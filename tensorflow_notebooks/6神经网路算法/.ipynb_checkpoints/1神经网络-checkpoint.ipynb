{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return 1.0 - np.tanh(x) * np.tanh(x)\n",
    "\n",
    "def logistic(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def logistic_derivative(x):\n",
    "    return logistic(x) * (1 - logistic(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "[0, 0] [-0.00462889]\n",
      "[0, 1] [ 0.99821608]\n",
      "[1, 0] [ 0.99801679]\n",
      "[1, 1] [-0.01832361]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "class NeuralNetwork:   \n",
    "    def __init__(self, layers, activation='tanh'):  \n",
    "        \"\"\"  \n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values  \n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"  \n",
    "        \"\"\"  \n",
    "        if activation == 'logistic':  \n",
    "            self.activation = logistic  \n",
    "            self.activation_deriv = logistic_derivative  \n",
    "        elif activation == 'tanh':  \n",
    "            self.activation = tanh  \n",
    "            self.activation_deriv = tanh_deriv\n",
    "    \n",
    "        self.weights = []  \n",
    "        for i in range(1, len(layers) - 1):  \n",
    "            self.weights.append((2*np.random.random((layers[i - 1] + 1, layers[i] + 1))-1)*0.25)  \n",
    "            self.weights.append((2*np.random.random((layers[i] + 1, layers[i + 1]))-1)*0.25)\n",
    "            \n",
    "            \n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=10000):         \n",
    "        X = np.atleast_2d(X)         \n",
    "        temp = np.ones([X.shape[0], X.shape[1]+1])         \n",
    "        temp[:, 0:-1] = X  # adding the bias unit to the input layer         \n",
    "        X = temp         \n",
    "        y = np.array(y)\n",
    "    \n",
    "        for k in range(epochs):  \n",
    "            i = np.random.randint(X.shape[0])  \n",
    "            a = [X[i]]\n",
    "    \n",
    "            for l in range(len(self.weights)):  #going forward network, for each layer\n",
    "                a.append(self.activation(np.dot(a[l], self.weights[l])))  #Computer the node value for each layer (O_i) using activation function\n",
    "            error = y[i] - a[-1]  #Computer the error at the top layer\n",
    "            deltas = [error * self.activation_deriv(a[-1])] #For output layer, Err calculation (delta is updated error)\n",
    "            \n",
    "            #Staring backprobagation\n",
    "            for l in range(len(a) - 2, 0, -1): # we need to begin at the second to last layer \n",
    "                #Compute the updated error (i,e, deltas) for each node going from top layer to input layer \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_deriv(a[l]))  \n",
    "            deltas.reverse()  \n",
    "            for i in range(len(self.weights)):  \n",
    "                layer = np.atleast_2d(a[i])  \n",
    "                delta = np.atleast_2d(deltas[i])  \n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "                \n",
    "                \n",
    "    def predict(self, x):         \n",
    "        x = np.array(x)         \n",
    "        temp = np.ones(x.shape[0]+1)         \n",
    "        temp[0:-1] = x         \n",
    "        a = temp         \n",
    "        for l in range(0, len(self.weights)):             \n",
    "            a = self.activation(np.dot(a, self.weights[l]))         \n",
    "        return a\n",
    "\n",
    "'''example one'''\n",
    "\n",
    "nn = NeuralNetwork([2,2,1],'tanh')\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,1,1,0])\n",
    "print(X)\n",
    "\n",
    "nn.fit(X,y)\n",
    "\n",
    "\n",
    "for i  in  [[0,0],[0,1],[1,0],[1,1]]:\n",
    "    print(i,nn.predict(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random() :  0.6031184749880928\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]]\n"
     ]
    }
   ],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
